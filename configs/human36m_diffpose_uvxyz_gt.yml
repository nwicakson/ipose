data:
    dataset: "human36m"
    dataset_path: "./data/data_3d_h36m.npz"
    dataset_path_train_2d: "./data/data_2d_h36m_gt_gmm.npz"
    dataset_path_test_2d: "./data/data_2d_h36m_gt_gmm.npz"
    num_joints: 17
    num_workers: 32

model:
    hid_dim: 96  # Keep original dimensions for compatibility
    emd_dim: 96
    coords_dim: [5,5]
    num_layer: 5
    n_head: 4
    dropout: 0.25
    n_pts: 17
    ema_rate: 0.9  # Reduced from 0.999 for faster adaptation
    ema: True
    resamp_with_conv: True
    var_type: fixedsmall
    implicit_layers: true
    implicit_start_layer: 4  # Only use one implicit layer
    implicit_max_iter: 1     # Start with 1 iteration
    implicit_max_iter_final: 15  # Reduced maximum iterations for stability
    implicit_tol: 0.05       # More relaxed tolerance

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.001
    num_diffusion_timesteps: 51  # Make sure this matches test_num_diffusion_timesteps

training:
    batch_size: 1024  # Larger batch size for stability
    n_epochs: 60
    n_iters: 5000000
    snapshot_freq: 5000
    validation_freq: 2000
    num_workers: 32
    mixed_precision: false
    implicit_warmup_epochs: 60  # Extended warmup period
    use_safe_testing: true      # Use the safer testing implementation

testing:
    test_times: 1              # Reduced from 5
    test_timesteps: 2           # Reduced from 50
    test_num_diffusion_timesteps: 10  # Reduced from 500
    implicit_refine_last: true
    enable_warmstart: false
    safe_batch_size: 16         # Smaller batch size for testing

optim:
    decay: 60
    optimizer: "Adam"
    lr: 0.000005  # Very low learning rate (5e-6)
    lr_gamma: 0.9
    amsgrad: true  # Enable amsgrad for Adam
    eps: 0.00000001
    grad_clip: 0.5  # More aggressive gradient clipping